{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Challenge - NLP\n",
    "\n",
    "BitTiger DS501\n",
    "\n",
    "Zhenning Tan 6/17/2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_dataset_challenge_round9/last_2_years_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>ave_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Restaurants, Cajun/Creole]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-28</td>\n",
       "      <td>0</td>\n",
       "      <td>iHP55csZHjPGqOMwIo70qQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Exceptional...exceptional steakhouse!! Ordered...</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>TU5j2S_Ub__ojLOpD_UepQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Restaurants, Cajun/Creole]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>GWI2xpBBwxK9-w1etLz51A</td>\n",
       "      <td>5</td>\n",
       "      <td>In a city with overrated 'celebrity' steakhous...</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>OC_WdUmY2fK-c1SD4JqSsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Restaurants, Cajun/Creole]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-30</td>\n",
       "      <td>0</td>\n",
       "      <td>CyZFXdnTCgpnKHNtIiKvaQ</td>\n",
       "      <td>3</td>\n",
       "      <td>The service was great. The appetizer bread was...</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>A6zYXofgFj6UhonFPrEDHw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Restaurants, Cajun/Creole]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>QxE_WJYBMsgzPk9ZBJ6bgA</td>\n",
       "      <td>5</td>\n",
       "      <td>Great service, great food, great environment. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>WHT6g24E7_B9aZiKgUgB6Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Restaurants, Cajun/Creole]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-15</td>\n",
       "      <td>0</td>\n",
       "      <td>gvvuzwPWHuRdKVv-P6OTRw</td>\n",
       "      <td>5</td>\n",
       "      <td>We were served chicken skewers, prosciutto, an...</td>\n",
       "      <td>review</td>\n",
       "      <td>0</td>\n",
       "      <td>Cn8UFE9uvIt-yFnASEmJnQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "2  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "3  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "4  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "\n",
       "                                 categories  ave_stars  cool        date  \\\n",
       "0  [Steakhouses, Restaurants, Cajun/Creole]        4.0     0  2016-07-28   \n",
       "1  [Steakhouses, Restaurants, Cajun/Creole]        4.0     0  2016-07-17   \n",
       "2  [Steakhouses, Restaurants, Cajun/Creole]        4.0     0  2016-10-30   \n",
       "3  [Steakhouses, Restaurants, Cajun/Creole]        4.0     0  2016-08-08   \n",
       "4  [Steakhouses, Restaurants, Cajun/Creole]        4.0     0  2016-10-15   \n",
       "\n",
       "   funny               review_id  stars  \\\n",
       "0      0  iHP55csZHjPGqOMwIo70qQ      5   \n",
       "1      0  GWI2xpBBwxK9-w1etLz51A      5   \n",
       "2      0  CyZFXdnTCgpnKHNtIiKvaQ      3   \n",
       "3      0  QxE_WJYBMsgzPk9ZBJ6bgA      5   \n",
       "4      0  gvvuzwPWHuRdKVv-P6OTRw      5   \n",
       "\n",
       "                                                text    type  useful  \\\n",
       "0  Exceptional...exceptional steakhouse!! Ordered...  review       0   \n",
       "1  In a city with overrated 'celebrity' steakhous...  review       0   \n",
       "2  The service was great. The appetizer bread was...  review       0   \n",
       "3  Great service, great food, great environment. ...  review       0   \n",
       "4  We were served chicken skewers, prosciutto, an...  review       0   \n",
       "\n",
       "                  user_id  \n",
       "0  TU5j2S_Ub__ojLOpD_UepQ  \n",
       "1  OC_WdUmY2fK-c1SD4JqSsw  \n",
       "2  A6zYXofgFj6UhonFPrEDHw  \n",
       "3  WHT6g24E7_B9aZiKgUgB6Q  \n",
       "4  Cn8UFE9uvIt-yFnASEmJnQ  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your feature variables, here is the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111548L,)\n",
      "Exceptional...exceptional steakhouse!! Ordered ribeye bone out and I've never had such an amazing cut of meat in my life! I died and went to heaven! The service is phenomenal! Had a huge party of about 15 and everyone was well attended too! The sides are superb too! Such a great atmosphere! Sophisticated an chic! Loved loved this place! A MUST when visiting Vegas!\n"
     ]
    }
   ],
   "source": [
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "print documents.shape\n",
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your target variable (any categorical variable that may be meaningful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, I am interested in perfect (5 stars) and imperfect (1-4 stars) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a column and take the values, save to a variable named \"target\"\n",
    "target = (df[\"stars\"] == 5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may want to look at the statistic of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56928\n",
       "1    54620\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Documents is your X, target is your y\n",
    "Now split the data to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split to documents_train, documents_test, target_train, target_test\n",
    "documents_train, documents_test, target_train, target_test = train_test_split(documents, target,\n",
    "                                                                              test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((78083L,), (33465L,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train.shape, documents_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\",\n",
    "                             max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "X_train = vectorizer.fit_transform(documents_train).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78083L, 1000L)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00', u'10', u'100', u'11', u'12', u'15', u'16', u'20', u'24', u'25', u'30', u'40', u'45', u'50', u'95', u'99', u'able', u'absolutely', u'accommodating', u'actually', u'add', u'added', u'affordable', u'afternoon', u'ago', u'ahead', u'amazing', u'ambiance', u'ambience', u'american', u'appetizer', u'appetizers', u'area', u'aren', u'arrived', u'asada', u'asian', u'ask', u'asked', u'asking', u'ate', u'atmosphere', u'attention', u'attentive', u'attitude', u'authentic', u'available', u'average', u'avocado', u'avoid', u'away', u'awesome', u'awful', u'ayce', u'bacon', u'bad', u'bag', u'baked', u'banana', u'bar', u'barely', u'bartender', u'based', u'basic', u'basically', u'bbq', u'bean', u'beans', u'beat', u'beautiful', u'beef', u'beer', u'beers', u'believe', u'bellagio', u'belly', u'benedict', u'best', u'better', u'big', u'birthday', u'bit', u'bite', u'bites', u'black', u'bland', u'blue', u'boba', u'bomb', u'bone', u'bottle', u'bowl', u'bowls', u'box', u'boy', u'boyfriend', u'bread', u'breakfast', u'bring', u'brisket']\n"
     ]
    }
   ],
   "source": [
    "# Get the vocab of your tfidf\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the trained model to transform your test data\n",
    "X_test = vectorizer.transform(documents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33465, 1000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will need these helper methods pretty soon\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  # np.argsort by default sorts values in ascending order\n",
    "\n",
    "def get_bottom_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the lowest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"mouse\", \"rabbit\"]\n",
    "    '''\n",
    "    return  [labels[i] for i in np.argsort(lst)[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Horrible service. We went there on a Tuesday night and it was nearly empty. The waiter was never checked on us I had to call every time we wanted to get more drinks fries order everything. Great food when it finally came but horrible service'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "unseen_review = np.random.choice(documents_test)\n",
    "unseen_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1L, 1000L)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the drawn review(s) to vector(s)\n",
    "unseen_review_vec = vectorizer.transform([unseen_review]).toarray()\n",
    "unseen_review_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "scores = cosine_similarity(X_train, unseen_review_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78083L, 1L)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"horrible, horrible, horrible.  food, service price.  it's an overpriced tourist trap.  Pease do yourself a favor, go to in n out.\",\n",
      " \"Went out for lunch on Christmas Eve with my family here and the service was horrible.  I figured since it was slow and no one was there the service would be great and the food would come out fast but it was the opposite!   The waiter didn't even ask us what we wanted to drink also we ordered a latte and it was COLD!   Then didn't even apologize for it or ask if we wanted something else it was horrible the server we had was name : Karina . Don't ever get her for your waiter ask for someone else , she wasn't every happy or cheerful overall it was a horrible service\",\n",
      " 'Horrible service. Horrible food. Appetizer came out AFTER our entrees. Do yourself a favor and go somewhere else',\n",
      " 'Horrible horrible horrible!!!! It was my sons birthday ruined by the horrible service and rude people that work here! We have been coming here for years never again!! NEVER',\n",
      " 'Wanted to try this place out again and they still have horrible service. Great food but service needs some improvement.']\n"
     ]
    }
   ],
   "source": [
    "# Let's find top 5 similar reviews\n",
    "n = 5\n",
    "pprint.pprint(get_top_values(scores.flatten(), n, list(documents_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search query:\n",
      "Horrible service. We went there on a Tuesday night and it was nearly empty. The waiter was never checked on us I had to call every time we wanted to get more drinks fries order everything. Great food when it finally came but horrible service\n"
     ]
    }
   ],
   "source": [
    "print 'Our search query:'\n",
    "print  unseen_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most 5 similar reviews:\n",
      "[\"horrible, horrible, horrible.  food, service price.  it's an overpriced tourist trap.  Pease do yourself a favor, go to in n out.\",\n",
      " \"Went out for lunch on Christmas Eve with my family here and the service was horrible.  I figured since it was slow and no one was there the service would be great and the food would come out fast but it was the opposite!   The waiter didn't even ask us what we wanted to drink also we ordered a latte and it was COLD!   Then didn't even apologize for it or ask if we wanted something else it was horrible the server we had was name : Karina . Don't ever get her for your waiter ask for someone else , she wasn't every happy or cheerful overall it was a horrible service\",\n",
      " 'Horrible service. Horrible food. Appetizer came out AFTER our entrees. Do yourself a favor and go somewhere else',\n",
      " 'Horrible horrible horrible!!!! It was my sons birthday ruined by the horrible service and rude people that work here! We have been coming here for years never again!! NEVER',\n",
      " 'Wanted to try this place out again and they still have horrible service. Great food but service needs some improvement.']\n"
     ]
    }
   ],
   "source": [
    "print 'Most %s similar reviews:' % n\n",
    "pprint.pprint(get_top_values(scores.flatten(), n, list(documents_train)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Does the result make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The result makes perfect sense. Both the query and returned results are about similar restaurants or food and they have the same sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Naive-Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80805040789928662"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "NB.score(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80693261616614376"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "NB.score(X_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=100, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_clf = LogisticRegression(random_state = 100)\n",
    "logistic_clf.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83034719465184481"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "logistic_clf.score(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82393545495293585"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "logistic_clf.score(X_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'best', u'awesome', u'incredible', u'delicious', u'thank', u'excellent', u'perfect', u'gem', u'great', u'perfection', u'disappoint', u'phenomenal', u'favorite', u'fantastic', u'highly', u'bomb', u'perfectly', u'love', u'greeted']\n"
     ]
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "print get_top_values(logistic_clf.coef_.flatten(), n, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'worst', u'horrible', u'rude', u'mediocre', u'slow', u'terrible', u'ok', u'bland', u'disappointing', u'okay', u'awful', u'unfortunately', u'overpriced', u'poor', u'dry', u'average', u'worse', u'charged', u'decent', u'soggy']\n"
     ]
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "print get_bottom_values(logistic_clf.coef_.flatten(), n, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: [u'worst',\n",
    " u'horrible',\n",
    " u'ok',\n",
    " u'slow',\n",
    " u'rude',\n",
    " u'mediocre',\n",
    " u'terrible',\n",
    " u'okay',\n",
    " u'disappointing',\n",
    " u'bland',\n",
    " u'unfortunately',\n",
    " u'overpriced',\n",
    " u'poor',\n",
    " u'awful',\n",
    " u'dry',\n",
    " u'lacking',\n",
    " u'average',\n",
    " u'decent',\n",
    " u'meh',\n",
    " u'reason']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=100,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state = 100)\n",
    "rf_clf.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99006185725445994"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "rf_clf.score(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77310623038995963"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "rf_clf.score(X_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What do you see from the training score and the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: test score is much worse than training score. This is due to overfitting. Use  cross validation to optimize the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Can you tell what features (words) are important by inspecting the RFC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'great', u'best', u'delicious', u'awesome', u'love', u'didn', u'bad', u'definitely', u'good', u'ok', u'vegas', u'just', u'favorite', u'friendly', u'place', u'excellent', u'wasn', u'worst', u'said']\n",
      "[u'clearly', u'speak', u'soy', u'placed', u'box', u'mistake', u'supposed', u'brown', u'selections', u'king', u'showed', u'tomatoes', u'pulled', u'window', u'known', u'asking', u'missing', u'menus', u'dont', u'mashed']\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "print get_top_values(rf_clf.feature_importances_, n, vocab)\n",
    "print get_bottom_values(rf_clf.feature_importances_, n, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with strong sentiment have high importance in the model. Neutral words or words without sentiment have low importance. However, in the low importance words, there are some words with negative sentiment, like \"crap\", \"wtf\", \"unprofessional\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #1: Use cross validation to evaluate your classifiers\n",
    "\n",
    "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82334486,  0.82160466,  0.82242572,  0.82594775,  0.82056865])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logistic_clf, X_train, target_train, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77609169,  0.77268361,  0.7722208 ,  0.77382172,  0.77196465])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf_clf, X_train, target_train, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation shows that the score on cross validation is similar to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit #2: Use grid search to find best predictable classifier\n",
    "\n",
    "\n",
    "[sklearn grid search tutorial (with cross validation)](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "[sklearn grid search documentation (with cross validation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=100, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l2', 'l1'], 'C': [0.5, 0.7, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"C\": [0.5, 0.7, 1],\n",
    "         \"penalty\": [\"l2\", \"l1\"]}\n",
    "gs_logistic = GridSearchCV(logistic_clf, param_grid = params, cv=5)\n",
    "gs_logistic.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.7, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=100, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.828976857959\n"
     ]
    }
   ],
   "source": [
    "print gs_logistic.best_estimator_\n",
    "print gs_logistic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.49439998,  1.56599998,  1.55580001,  1.71240005,  1.62759995,\n",
       "         1.78979993]),\n",
       " 'mean_score_time': array([ 0.05619993,  0.05680003,  0.05280004,  0.05900002,  0.05799999,\n",
       "         0.06100006]),\n",
       " 'mean_test_score': array([ 0.82840055,  0.82865669,  0.82845178,  0.82897686,  0.82841336,\n",
       "         0.82882318]),\n",
       " 'mean_train_score': array([ 0.83977626,  0.83763751,  0.84108896,  0.84036217,  0.84212632,\n",
       "         0.84199825]),\n",
       " 'param_C': masked_array(data = [0.5 0.5 0.7 0.7 1 1],\n",
       "              mask = [False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l2' 'l1' 'l2' 'l1' 'l2' 'l1'],\n",
       "              mask = [False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 0.5, 'penalty': 'l2'},\n",
       "  {'C': 0.5, 'penalty': 'l1'},\n",
       "  {'C': 0.7, 'penalty': 'l2'},\n",
       "  {'C': 0.7, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 1, 'penalty': 'l1'}),\n",
       " 'rank_test_score': array([6, 3, 4, 1, 5, 2]),\n",
       " 'split0_test_score': array([ 0.83346139,  0.83275708,  0.83397362,  0.83307722,  0.83422973,\n",
       "         0.8328211 ]),\n",
       " 'split0_train_score': array([ 0.83898183,  0.83666053,  0.83986232,  0.83938205,  0.84120708,\n",
       "         0.8408869 ]),\n",
       " 'split1_test_score': array([ 0.82487033,  0.82762374,  0.82525453,  0.82659922,  0.8251905 ,\n",
       "         0.82615099]),\n",
       " 'split1_train_score': array([ 0.83964076,  0.83824801,  0.84090545,  0.84111357,  0.84212211,\n",
       "         0.84249031]),\n",
       " 'split2_test_score': array([ 0.828125  ,  0.82774078,  0.82825307,  0.82806096,  0.82774078,\n",
       "         0.82742059]),\n",
       " 'split2_train_score': array([ 0.84050779,  0.83783438,  0.84214065,  0.84041174,  0.84271695,\n",
       "         0.84233275]),\n",
       " 'split3_test_score': array([ 0.8310707 ,  0.83119877,  0.8305584 ,  0.83209529,  0.83075051,\n",
       "         0.83119877]),\n",
       " 'split3_train_score': array([ 0.83881089,  0.83615349,  0.84039573,  0.83929115,  0.84167641,\n",
       "         0.8411001 ]),\n",
       " 'split4_test_score': array([ 0.8244749 ,  0.8239626 ,  0.82421875,  0.82505123,  0.82415471,\n",
       "         0.82652408]),\n",
       " 'split4_train_score': array([ 0.84094002,  0.83929115,  0.84214065,  0.84161237,  0.84290906,\n",
       "         0.8431812 ]),\n",
       " 'std_fit_time': array([ 0.01656022,  0.01466968,  0.00604647,  0.07906601,  0.01700119,\n",
       "         0.10010077]),\n",
       " 'std_score_time': array([ 0.00421423,  0.00661512,  0.0029258 ,  0.00493965,  0.00641876,\n",
       "         0.00379472]),\n",
       " 'std_test_score': array([ 0.00348404,  0.00307298,  0.00355278,  0.00311258,  0.00369514,\n",
       "         0.00268409]),\n",
       " 'std_train_score': array([ 0.00083317,  0.00112275,  0.00091989,  0.00092063,  0.00063387,\n",
       "         0.00087123])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logistic.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300014940983117"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logistic.best_estimator_.score(X_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'best', u'incredible', u'heaven', u'awesome', u'thank', u'delicious', u'gem', u'perfect', u'disappoint', u'excellent', u'phenomenal', u'great', u'perfection', u'bomb', u'soooo', u'fantastic', u'favorite', u'sooooo', u'highly']\n",
      "[u'worst', u'horrible', u'disappointing', u'rude', u'mediocre', u'terrible', u'slow', u'bland', u'ok', u'lacking', u'awful', u'okay', u'poor', u'meh', u'unfortunately', u'disgusting', u'worse', u'overpriced', u'charged', u'nasty']\n"
     ]
    }
   ],
   "source": [
    "print get_top_values(gs_logistic.best_estimator_.coef_.flatten(), n, vocab)\n",
    "print get_bottom_values(gs_logistic.best_estimator_.coef_.flatten(), n, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much improvement for logistic regression after screen C and penalty. However, through cross validation, we can see how the classifier performs with different parameters. This gives us confidence on our model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used: 358.618999958 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "params = {\"n_estimators\": [30, 50, 100],\n",
    "         \"max_depth\":[15],\n",
    "         \"max_features\": [30]}\n",
    "gs_rf = GridSearchCV(rf_clf, param_grid = params, cv=5)\n",
    "gs_rf.fit(X_train, target_train)\n",
    "print \"time used:\", time()-t0, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=15, max_features=30, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=1, oob_score=False, random_state=100,\n",
      "            verbose=0, warm_start=False)\n",
      "0.784216794949\n"
     ]
    }
   ],
   "source": [
    "print gs_rf.best_estimator_\n",
    "print gs_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 10.08859997,  16.5928    ,  32.62979999]),\n",
       " 'mean_score_time': array([ 0.15960007,  0.23779998,  0.4382    ]),\n",
       " 'mean_test_score': array([ 0.78022105,  0.78298733,  0.78421679]),\n",
       " 'mean_train_score': array([ 0.81705685,  0.81972388,  0.82251578]),\n",
       " 'param_max_depth': masked_array(data = [15 15 15],\n",
       "              mask = [False False False],\n",
       "        fill_value = ?),\n",
       " 'param_max_features': masked_array(data = [30 30 30],\n",
       "              mask = [False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [30 50 100],\n",
       "              mask = [False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'max_depth': 15, 'max_features': 30, 'n_estimators': 30},\n",
       "  {'max_depth': 15, 'max_features': 30, 'n_estimators': 50},\n",
       "  {'max_depth': 15, 'max_features': 30, 'n_estimators': 100}),\n",
       " 'rank_test_score': array([3, 2, 1]),\n",
       " 'split0_test_score': array([ 0.78153413,  0.78422333,  0.78755282]),\n",
       " 'split0_train_score': array([ 0.81648923,  0.81876251,  0.82182022]),\n",
       " 'split1_test_score': array([ 0.78055965,  0.78273676,  0.78395338]),\n",
       " 'split1_train_score': array([ 0.81684436,  0.81935773,  0.82303973]),\n",
       " 'split2_test_score': array([ 0.7824667 ,  0.78349129,  0.78336322]),\n",
       " 'split2_train_score': array([ 0.81577473,  0.81811196,  0.82118559]),\n",
       " 'split3_test_score': array([ 0.78163422,  0.78464395,  0.78445184]),\n",
       " 'split3_train_score': array([ 0.81692734,  0.82136168,  0.82214609]),\n",
       " 'split4_test_score': array([ 0.77491035,  0.77984119,  0.7817623 ]),\n",
       " 'split4_train_score': array([ 0.81924856,  0.8210255 ,  0.82438728]),\n",
       " 'std_fit_time': array([ 0.0534438 ,  0.03599112,  0.08588219]),\n",
       " 'std_score_time': array([ 0.0049234 ,  0.00577587,  0.00444524]),\n",
       " 'std_test_score': array([ 0.00272328,  0.00170216,  0.00189777]),\n",
       " 'std_train_score': array([ 0.00116889,  0.00126753,  0.00111058])}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many factors to tune in random forest model, including the number of trees, maximum features on split, tree depth etc . Due to the limitation of my computation power, I am not able to fully explore the grid. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
